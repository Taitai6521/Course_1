{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Stocks Sentiment Analysis Using AI - Solution.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0LUX0LaYmKNa"},"source":["#task #1: 問題提起とビジネスケースを理解する"]},{"cell_type":"markdown","metadata":{"id":"Zm43sLloJnSQ"},"source":["![alt text](https://drive.google.com/uc?id=1djxup79_KiGtKFiH7AgSD0Bj-2D90TBg)"]},{"cell_type":"markdown","metadata":{"id":"_n-3qhTpXjsy"},"source":["#task #2: ライブラリ/データセットのインポートと探索的データ分析の実行"]},{"cell_type":"code","metadata":{"id":"bpiddPjsl_4Q"},"source":["# import key libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud, STOPWORDS\n","import nltk\n","import re\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","import plotly.express as px\n","\n","# Tensorflow\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hWWbtOTf4kS-"},"source":["# Mount the google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmtxigpoQZh4"},"source":["# install nltk\n","# NLTK: Natural Language tool kit\n","!pip install nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaWz3q0OQg6h"},"source":["# install gensim\n","# Gensimは、教師なしのトピックモデリングと自然言語処理のためのオープンソースのライブラリです。\n","# Gensim は Python と Cython で実装されています。\n","\n","!pip install gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QiTczEunJNx"},"source":["# 株式ニュースデータの読み込み\n","stock_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Python & ML in Finance/stock_sentiment.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DB2gQ1w4nR-P"},"source":["# データセットを表示してみよう \n","stock_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mEVxeFq5Zlsd"},"source":["# データフレームの情報\n","stock_df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iH5W6t7fZoQS"},"source":["# null値のチェック\n","stock_df.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-sKr-33Y8YM"},"source":["**ミニ・チャレンジ #1:**\n","\n","- **「sentiment」列にはユニークな要素がいくつありますか？2つの異なる方法で調べてみてください。\n"]},{"cell_type":"code","metadata":{"id":"cjtge71bZxhw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgkZHO3CPnsp"},"source":["#task #3: データクリーニングを行う (テキストから句読点を取り除く)"]},{"cell_type":"code","metadata":{"id":"BAWaIZEd6cYj"},"source":["import string\n","string.punctuation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbWfzII36cWc"},"source":["Test = '$I love AI & Machine learning!!'\n","Test_punc_removed = [char for char in Test if char not in string.punctuation]\n","Test_punc_removed_join = ''.join(Test_punc_removed)\n","Test_punc_removed_join"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EvhT_EjX6cUQ"},"source":["Test = 'Good morning beautiful people :)... #I am having fun learning Finance with Python!!'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WM76PxJ36cSQ"},"source":["Test_punc_removed = [char for char in Test if char not in string.punctuation]\n","Test_punc_removed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmG7ijks6aqt"},"source":["# 再び文字を結合して文字列を形成する。\n","Test_punc_removed_join = ''.join(Test_punc_removed)\n","Test_punc_removed_join"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HE4dRfZG_O8O"},"source":["# 句読点を除去する関数を定義しよう\n","def remove_punc(message):\n","    Test_punc_removed = [char for char in message if char not in string.punctuation]\n","    Test_punc_removed_join = ''.join(Test_punc_removed)\n","\n","    return Test_punc_removed_join"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9KW-IgnK-dV4"},"source":["# データセットから句読点を削除してみよう \n","stock_df['Text Without Punctuation'] = stock_df['Text'].apply(remove_punc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OU1Ubd45_3t-"},"source":["stock_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GklP2yj5Ab_D"},"source":["stock_df['Text'][2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ohgg1iNHAe4W"},"source":["stock_df['Text Without Punctuation'][2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YnSDZVMH7Joa"},"source":["**ミニチャレンジ #2:** \n","- **別の方法で句読点を削除する**。\n"]},{"cell_type":"code","metadata":{"id":"LRjfAhXL68XV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2w1Rm4Zm68Zi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S2fMUtl66ztE"},"source":["# task #4: データクリーニング（ストップワードの削除）を行う"]},{"cell_type":"code","metadata":{"id":"YmEkZj3k69S7"},"source":["# ストップワードのダウンロード\n","nltk.download(\"stopwords\")\n","stopwords.words('english')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyDMuSWA69Y2"},"source":["# nltkから追加のストップワードを取得する\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'will', 'aap', 'co', 'day', 'user', 'stock', 'today', 'week', 'year'])\n","# stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'will', 'aap', 'co', 'day', 'user', 'stock','today', 'week', 'year', 'https'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jy1n42a069Vr"},"source":["# ストップワードの除去と短い単語（2文字以下）の除去\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if len(token) >= 3 and token not in stop_words:\n","            result.append(token)\n","            \n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zv1Igmf87a5I"},"source":["# text列に前処理を施す\n","stock_df['Text Without Punc & Stopwords'] = stock_df['Text Without Punctuation'].apply(preprocess)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCgsA4vG9kxj"},"source":["stock_df['Text'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-WDumvb93wC"},"source":["stock_df['Text Without Punc & Stopwords'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bYql7en7hcs"},"source":["# 単語を文字列に結合\n","# stock_df['Processed Text 2'] = stock_df['Processed Text 2'].apply(lambda x: \" \".join(x))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9JqZBzf9CEY"},"source":["stock_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pA4VMxIf7mha"},"source":["**ミニチャレンジ #3:**\n","\n","- **3文字ではなく2文字以上の単語を保持するようにコードを修正する**。\n","- **ストップワードのリストに'https'を追加し、コードを再実行する**。\n"]},{"cell_type":"code","metadata":{"id":"3arHVxiNCFIA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7RUFuufl-Hx8"},"source":["# task #5: plot wordcloud"]},{"cell_type":"code","metadata":{"id":"cJAYGiUwg-Ez"},"source":["# 単語を文字列に結合する\n","stock_df['Text Without Punc & Stopwords Joined'] = stock_df['Text Without Punc & Stopwords'].apply(lambda x: \" \".join(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ErWl2jf7hYS"},"source":["# 肯定的な感情を持つテキストのワードクラウドをプロットする\n","plt.figure(figsize = (20, 20)) \n","wc = WordCloud(max_words = 1000 , width = 1600 , height = 800).generate(\" \".join(stock_df[stock_df['Sentiment'] == 1]['Text Without Punc & Stopwords Joined']))\n","plt.imshow(wc, interpolation = 'bilinear');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Vt7d0CqVO4H"},"source":["**ミニチャレンジ#4:**\n","- **ネガティブな感情を持つツイートのワードクラウドを可視化する**。\n"]},{"cell_type":"markdown","metadata":{"id":"Q2m28Qrtds_n"},"source":["# task #6: クリーニングされたデータセットの可視化"]},{"cell_type":"code","metadata":{"id":"eB8NRW0fIQRj"},"source":["stock_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u93O9tHHiRfd"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgVLvPBLiZGb"},"source":["# word_tokenizeは、文字列を単語に分解するために使います\n","print(stock_df['Text Without Punc & Stopwords Joined'][0])\n","print(nltk.word_tokenize(stock_df['Text Without Punc & Stopwords Joined'][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r20ny06ECP1B"},"source":["# ドキュメント内のデータの最大長を取得します。\n","# これは後で単語埋め込みを生成する際に使用される\n","\n","maxlen = -1\n","for doc in stock_df['Text Without Punc & Stopwords Joined']:\n","    tokens = nltk.word_tokenize(doc)\n","    if(maxlen < len(tokens)):\n","        maxlen = len(tokens)\n","print(\"The maximum number of words in any document is:\", maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffCdD2BZjFyP"},"source":["tweets_length = [ len(nltk.word_tokenize(x)) for x in stock_df['Text Without Punc & Stopwords Joined'] ]\n","tweets_length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDtUZMRVcD7I"},"source":["# テキスト中の単語数の分布をプロットする\n","fig = px.histogram(x = tweets_length, nbins = 50)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EPTNSE6_Xd-b"},"source":["**ミニチャレンジ #5:**\n","- **Seaborn Countplotを使って、肯定的な感情と否定的な感情のクラスに属するサンプルの数を視覚的に示してください**。\n"]},{"cell_type":"code","metadata":{"id":"aaj3nkiaYHLX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zi3neznbP_QT"},"source":["# task #7: トークン化とパディングによるデータの準備"]},{"cell_type":"markdown","metadata":{"id":"A0MfgDJ8K-Jm"},"source":["![alt text](https://drive.google.com/uc?id=13j8m-JOpK994CtukR1EShiY_hGGjkNx-)"]},{"cell_type":"code","metadata":{"id":"_ZHGI_qYj6Iw"},"source":["stock_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"alPKDD0P7a28"},"source":["# データセットに含まれる単語の総数を得る\n","list_of_words = []\n","for i in stock_df['Text Without Punc & Stopwords']:\n","    for j in i:\n","        list_of_words.append(j)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FN75gigvYVxc"},"source":["list_of_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IN5g6jTF7a1I"},"source":["# ユニークな単語の総数を得る\n","total_words = len(list(set(list_of_words)))\n","total_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUgEf-SZ7R7c"},"source":["# データをテストとトレーニングに分ける \n","X = stock_df['Text Without Punc & Stopwords']\n","y = stock_df['Sentiment']\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgFE9ss1JmCw"},"source":["X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NeoabpYjnS3o"},"source":["X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcyTkiaZlQCK"},"source":["X_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kme-IYsM6uJa"},"source":["# 単語をトークン化するトークナイザーを作成し，トークン化された単語のシーケンスを作成する\n","tokenizer = Tokenizer(num_words = total_words)\n","tokenizer.fit_on_texts(X_train)\n","\n","# トレーニングデータ\n","train_sequences = tokenizer.tests_to_sequences(X_train)\n","\n","# テストデータ\n","test_sequences = tokenizer.tests_to_sequences(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4tGqwPkoiXea"},"source":["train_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3joXOZeieD0"},"source":["test_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TEWCpgXYNF1r"},"source":["print(\"The encoding for document\\n\", X_train[1:2],\"\\n is: \", train_sequences[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"URy4Wkai_Qh3"},"source":["# トレーニングとテストにパディングを追加\n","padded_train = pad_sequences(train_sequences, maxlen = 29, padding = 'post', truncating = 'post')\n","padded_test = pad_sequences(test_sequences, maxlen = 29, truncating = 'post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D60mkoZvSG5D"},"source":["for i, doc in enumerate(padded_train[:3]):\n","     print(\"The padded encoding for document:\", i+1,\" is:\", doc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KlUjvz5UPZm0"},"source":["# データをカテゴライズされた2次元表現に変換する\n","y_train_cat = to_categorical(y_train, 2)\n","y_test_cat = to_categorical(y_test, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqPAOMmfPhxm"},"source":["y_train_cat.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MYO-dWSjIHq"},"source":["y_test_cat.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"und7m_TDjGf3"},"source":["y_train_cat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zL_1IF0xo2x6"},"source":["**ミニチャレンジ #6:**\n","\n","- パディングの長さを15に変更して、コードを再実行します。パディングが成功したことを確認する**。\n"]},{"cell_type":"code","metadata":{"id":"DLpwzbPwo2b9"},"source":["# トレーニングとテストにパディングを追加する\n","padded_train = pad_sequences(train_sequences, maxlen = 15, padding = 'post', truncating = 'post')\n","padded_test = pad_sequences(test_sequences, maxlen = 15, truncating = 'post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yOufWm8yjaoW"},"source":["# task #8: リカレントニューラルネットワークと長短記憶ネットワーク(lstm)の理論と直感を理解する"]},{"cell_type":"markdown","metadata":{"id":"hVsilx4zPUIj"},"source":["![alt text](https://drive.google.com/uc?id=1Giaz7q1THBFTuNFpSyLBKnoUbbvWlNw3)"]},{"cell_type":"markdown","metadata":{"id":"-KOnwBjvPdFj"},"source":["![alt text](https://drive.google.com/uc?id=1iDKpQqmGTNr3riuQOvXdiwfy9wlCU5st)"]},{"cell_type":"markdown","metadata":{"id":"SU_qrPe5Pmkd"},"source":["![alt text](https://drive.google.com/uc?id=1PxW6DBer4d1Q9_9OSaAQDTtqUdDGLdYa)"]},{"cell_type":"markdown","metadata":{"id":"n-9Q8dOojRIf"},"source":["# task #9: センチメント分析を行うカスタムベースのディープニューラルネットワークを構築する"]},{"cell_type":"markdown","metadata":{"id":"eMzQsewkLKAn"},"source":["![alt text](https://drive.google.com/uc?id=1zpI1XHM1CSxLPjsW7QTahfs_f2stzKeQ)"]},{"cell_type":"code","metadata":{"id":"3k_ZJfGLjQhc"},"source":["# シーケンシャルモデル\n","model = Sequential()\n","\n","# エンベッディング層\n","model.add(Embedding(total_words, output_dim = 512))\n","\n","# 双方向RNNとLSTM\n","model.add(LSTM(256))\n","\n","# 密な層\n","model.add(Dense(128, activation = 'relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(2,activation = 'softmax'))\n","model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhbQTkvfjNVi"},"source":["# モデルの学習\n","model.fit(padded_train, y_train_cat, batch_size = 32, validation_split = 0.2, epochs = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"teWbFh2eqbIA"},"source":["**ミニチャレンジ #7:**\n","- **異なるエンベッディング出力次元を用いてモデルをトレーニングする**。\n"]},{"cell_type":"code","metadata":{"id":"_kc3H3LRjrIp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9UIM9ugjrb0"},"source":["# task #10: 学習したモデルの性能を評価する"]},{"cell_type":"markdown","metadata":{"id":"bAZtLrGTPvlx"},"source":["![alt text](https://drive.google.com/uc?id=1MZdb0g69XDC4JRATR9K6-2NAkrclGAXO)"]},{"cell_type":"code","metadata":{"id":"7qRUkys-BSuQ"},"source":["# 予測\n","pred = model.predict(padded_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qh66RfZgF7ln"},"source":["# 予測の作成\n","prediction = []\n","for i in pred:\n","  prediction.append(np.argmax(i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Z3ka_OTQK6G"},"source":["# 元の値を含むリスト\n","original = []\n","for i in y_test_cat:\n","  original.append(np.argmax(i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxjH9_yZQj_m"},"source":["# テキストデータの精度スコア\n","from sklearn.metrics import accuracy_score\n","\n","accuracy = accuracy_score(original, prediction)\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rpgzbSqHfR4"},"source":["# 混同行列のプロット\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(original, prediction)\n","sns.heatmap(cm, annot = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KYY2JDUVuL64"},"source":["**ミニチャレンジ #8:**\n","\n","- **事前に学習したBERTモデルを使用して、感情分析の予測を行う**。\n"]},{"cell_type":"code","metadata":{"id":"Mhwl15DAucKD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54LObFe0Y-vB"},"source":["# **EXCELLENT JOB!**"]},{"cell_type":"markdown","metadata":{"id":"wNuSau6LZC9I"},"source":["**ミニチャレンジ #8:**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7IRjyZV-Yhdl"},"source":["**ミニチャレンジ #1 解決策:**\n","\n","- **「sentiment」列には、いくつのユニークな要素がありますか？2つの異なる方法で調べてみましょう。\n","sns.countplot(stock_df['Sentiment'])\n"]},{"cell_type":"code","metadata":{"id":"imlxP0Q9YcR3"},"source":["sns.countplot(stock_df['Sentiment'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MX8eMi_v47u5"},"source":["# 特定の列に存在するユニークな値の数を求める\n","stock_df['Sentiment'].nunique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lid7lewH7GYZ"},"source":["**MINI CHALLENGE #2 SOLUTION:** \n","- **Remove punctuations using a different method**\n"]},{"cell_type":"code","metadata":{"id":"hfkdX7N-6bHD"},"source":["fTest_punc_removed = []\n","for char in Test: \n","    if char not in string.punctuation:\n","        Test_punc_removed.append(char)\n","\n","# 再び文字を結合して文字列を形成する。  \n","Test_punc_removed_join = ''.join(Test_punc_removed)\n","Test_punc_removed_join"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5qBbEYEtTadZ"},"source":["**ミニチャレンジ#3の解決策：***。\n","\n","- 3文字ではなく2文字以上の単語を保持するようにコードを修正する**。\n","- ストップワードのリストに'https'を追加してコードを再実行する**。\n"]},{"cell_type":"code","metadata":{"id":"-MwBYHnhTZYR"},"source":["# nltkから追加のストップワードを取得する\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","stop_words.extend(['from', 'subject', 're', 'edu', 'use','will','aap','co','day','user','stock','today','week','year', 'https'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KIxqGCFvTlQC"},"source":["# ストップワードを除去し、2文字以下の単語を削除する\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >= 2 and token not in stop_words:\n","            result.append(token)\n","            \n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMqaQhvXVxDm"},"source":["**ミニチャレンジ #4 解決策：***。\n","- **ネガティブな感情を持つツイートのワードクラウドを可視化する**。\n"]},{"cell_type":"code","metadata":{"id":"hZoc-8qR7ayv"},"source":["# 否定的な感情を持つテキストのワードクラウドをプロットする\n","plt.figure(figsize = (20,20)) \n","wc = WordCloud(max_words = 1000, width = 1600, height = 800 ).generate(\" \".join(stock_df[stock_df['Sentiment'] == 0]['Text Without Punc & Stopwords Joined']))\n","plt.imshow(wc, interpolation = 'bilinear');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0FRzUbePXB4q"},"source":["**ミニチャレンジ#5の解決策：***。\n","- **Seaborn Countplotを使って、肯定的な感情と否定的な感情に属するサンプルの数を視覚的に示します**。\n"]},{"cell_type":"code","metadata":{"id":"DoRESyH1dxiw"},"source":["# 単語数をプロットする\n","sns.countplot(stock_df['Sentiment'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VegekE5ppO4A"},"source":["**ミニチャレンジ #6 解決策:**\n","\n","- パディングの長さを15に変更して、コードを再実行します。パディングが成功したことを確認する**。\n"]},{"cell_type":"code","metadata":{"id":"kkxyi3jtpO4E"},"source":["# トレーニングとテストにパディングを追加\n","padded_train = pad_sequences(train_sequences, maxlen = 15, padding = 'post', truncating = 'post')\n","padded_test = pad_sequences(test_sequences, maxlen = 15, truncating = 'post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCrSJiesquw1"},"source":["**ミニチャレンジ#7の解決策：***。\n","- 異なるエンベッディング出力次元を使ってモデルをトレーニングする**。\n"]},{"cell_type":"code","metadata":{"id":"CzEEVItSquw_"},"source":["model = Sequential()\n","\n","# エンベッディング層\n","model.add(Embedding(total_words, output_dim = 256))\n","\n","# 双方向性RNNとLSTM\n","model.add(Bidirectional(LSTM(128)))\n","\n","# 密な層\n","model.add(Dense(128, activation = 'relu'))\n","model.add(Dense(1,activation = 'sigmoid'))\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gZAKgzEgufIL"},"source":["**ミニチャレンジ #8 解決策：***。\n","\n","- **事前に学習させたBERTモデルを使用して、感情分析の予測を行う**。\n"]},{"cell_type":"code","metadata":{"id":"sU_SxUJQu_iA"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJrZVwVcXivM"},"source":["# 特定のタスクを実行するためにトランスフォーマーからのパイプラインを使用します。\n","# タスクとしてセンチメント分析を指定し、文字列を渡すと、結果が得られます。\n","# トピックモデリング、Q&A、テキスト要約などのタスクをここで指定することができます。\n","from transformers import pipeline\n","\n","nlp = pipeline('s sentiment-analysis')\n","\n","# テストデータで予測を行う\n","pred = nlp(list(X_test))\n","\n","# 予測値は辞書なので、ラベルをdictから取得する\n","予測値 = [].\n","for i in pred:\n","  prediction.append(i['label'])\n","\n","# 最終結果を表示する\n","for i in range(len(prediction[:3])):\n","  print(\"˶ˆ꒳ˆ˵\", df[df.combined == X_test.values[i]].Text.item(), \"˶ˆ꒳ˆ˵\", \"˶ˆ꒳ˆ˵\", \"˶ˆ꒳ˆ˵\", \"˶ˆ꒳ˆ˵\",\n","      category[df[df.combined == X_test.values[i]].Sentiment.item()], \"˶nnPredicted value :˶n˶n˶n\", prediction[i], \"˶n˶n˶n˶n˶n˶n˶n˶\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3NRMvwi5vCgR"},"source":[""],"execution_count":null,"outputs":[]}]}